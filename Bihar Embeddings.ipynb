{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be584a17-170b-47fc-b50a-20f2ef37d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SpeechBrain file\n",
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from speechbrain.pretrained.interfaces import foreign_class\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "input_folder = '/home/sarthak/CAPSTONE_PROJECT/DRISHTI_SINGH/Dataset/bihar_files/'  # Replace with your folder path\n",
    "output_folder = '/home/sarthak/CAPSTONE_PROJECT/DRISHTI_SINGH/Dataset/SpeechBrain_bihar_files'  # Replace with your output folder path\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = foreign_class(\n",
    "    source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\",\n",
    "    pymodule_file=\"custom_interface.py\",\n",
    "    classname=\"CustomEncoderWav2vec2Classifier\"\n",
    ")\n",
    "\n",
    "def extract_features(path):\n",
    "    # Load the audio file\n",
    "    signal, fs = torchaudio.load(path)\n",
    "    # Move the signal to the GPU\n",
    "    signal = signal.to(device)\n",
    "    # Get embeddings\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    # Move embeddings back to CPU\n",
    "    embeddings = embeddings.cpu()\n",
    "    # Average embeddings and return\n",
    "    return np.array(embeddings.mean(axis=0).squeeze())\n",
    "    \n",
    "# Get list of .flac files\n",
    "flac_files = [file for file in os.listdir(input_folder) if file.endswith('.flac')]\n",
    "\n",
    "# Process each file with a progress bar\n",
    "for file_name in tqdm(flac_files, desc=\"Processing files\", unit=\"file\"):\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    embeddings = extract_features(file_path)\n",
    "    \n",
    "    # Save embeddings as .npy\n",
    "    output_path = os.path.join(output_folder, file_name.replace('.flac', '.npy'))\n",
    "    np.save(output_path, embeddings)\n",
    "    print(f\"Saved embeddings for {file_name} to {output_path}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f059010-c359-4add-9052-114fd5e35c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wav2Vec2.0 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ed041-a8f4-4e87-b088-a54163869028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Paths\n",
    "input_folder = '/home/sarthak/CAPSTONE_PROJECT/DRISHTI_SINGH/Dataset/bihar_files/'  # Replace with your folder path\n",
    "output_folder = '/home/sarthak/CAPSTONE_PROJECT/DRISHTI_SINGH/Dataset/Embeddings_wav2vec2_bihar'  # Replace with your output folder path\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Wave2Vec 2.0 model and processor\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(file_path):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=16000)  # Ensure 16kHz sampling rate\n",
    "    \n",
    "    # Process audio with the processor\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move tensors to the GPU\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Convert embeddings to CPU and numpy\n",
    "    embeddings = outputs.last_hidden_state.squeeze().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Get list of .flac files\n",
    "flac_files = [file for file in os.listdir(input_folder) if file.endswith('.flac')]\n",
    "\n",
    "# Process each file with a progress bar\n",
    "for file_name in tqdm(flac_files, desc=\"Processing files\", unit=\"file\"):\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    embeddings = extract_embeddings(file_path)\n",
    "    \n",
    "    # Save embeddings as .npy\n",
    "    output_path = os.path.join(output_folder, file_name.replace('.flac', '.npy'))\n",
    "    np.save(output_path, embeddings)\n",
    "    print(f\"Saved embeddings for {file_name} to {output_path}\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204e7c0-266d-4b64-83ab-04073b254aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
